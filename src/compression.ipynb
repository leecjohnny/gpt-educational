{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johnny/Repos/gpt-educational/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "import tiktoken\n",
    "from pathlib import Path\n",
    "\n",
    "import zstd\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "enc = tiktoken.get_encoding(\"o200k_base\")\n",
    "\n",
    "tinyshakespeare = Path(\"../data/tinyshakespeare_input.txt\").read_text()\n",
    "\n",
    "\n",
    "splits = {\n",
    "    \"test\": \"all/test-00000-of-00001.parquet\",\n",
    "    \"validation\": \"all/validation-00000-of-00001.parquet\",\n",
    "    \"dev\": \"all/dev-00000-of-00001.parquet\",\n",
    "    \"auxiliary_train\": \"all/auxiliary_train-00000-of-00001.parquet\",\n",
    "}\n",
    "df = pd.read_parquet(\"hf://datasets/cais/mmlu/\" + splits[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def make_text_string_for_mmlu(question, choices):\n",
    "    text_template = f\"\"\"\n",
    "    Question: {question}\n",
    "    A. {choices[0]}\n",
    "    B. {choices[1]}\n",
    "    C. {choices[2]}\n",
    "    D. {choices[3]}\n",
    "    \"\"\"\n",
    "    return text_template\n",
    "\n",
    "\n",
    "sample_df = df.sample(frac=1, random_state=42)[:5000]\n",
    "train_df = sample_df.iloc[:4900]\n",
    "test_df = sample_df.iloc[4900:]\n",
    "\n",
    "test_set = [\n",
    "    (make_text_string_for_mmlu(row[\"question\"], row[\"choices\"]), row[\"answer\"])\n",
    "    for _, row in test_df.iterrows()\n",
    "]\n",
    "training_set = [\n",
    "    (make_text_string_for_mmlu(row[\"question\"], row[\"choices\"]), row[\"answer\"])\n",
    "    for _, row in train_df.iterrows()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42000"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digit_recognizer = pd.read_csv(\"../data/digit-recognizer-train.csv\")\n",
    "len(digit_recognizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import io\n",
    "\n",
    "\n",
    "def make_digit_pil_image(row):\n",
    "    # Convert pixel values from 0 to 255 (grayscale)\n",
    "    image = row.iloc[1:].values.reshape(28, 28).astype(np.uint8)\n",
    "    # Create PIL image in \"L\" mode (8-bit grayscale, 0-255)\n",
    "    image = Image.fromarray(image, mode=\"L\")\n",
    "    return image\n",
    "\n",
    "\n",
    "images = digit_recognizer.apply(make_digit_pil_image, axis=1)\n",
    "class_labels = digit_recognizer[\"label\"]\n",
    "image_bytes = images.apply(lambda x: io.BytesIO(x.tobytes()).getvalue())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of image_bytes[0]: 784\n",
      "length of gzip_image_bytes: 170\n",
      "length of zstd_image_bytes: 174\n",
      "length of tokenized_image_bytes: 444\n"
     ]
    }
   ],
   "source": [
    "print(f\"length of image_bytes[0]: {len(image_bytes[0])}\")\n",
    "##gzip\n",
    "gzip_image_bytes = gzip.compress(image_bytes[0])\n",
    "print(f\"length of gzip_image_bytes: {len(gzip_image_bytes)}\")\n",
    "##zstd\n",
    "zstd_image_bytes = zstd.compress(image_bytes[0])\n",
    "print(f\"length of zstd_image_bytes: {len(zstd_image_bytes)}\")\n",
    "\n",
    "##tokenizer\n",
    "tokenizer = enc\n",
    "tokenized_image_bytes = tokenizer._encode_bytes(image_bytes[0])\n",
    "print(f\"length of tokenized_image_bytes: {len(tokenized_image_bytes)}\")\n",
    "\n",
    "entire_dataset = list(zip(image_bytes, class_labels))[:5000]\n",
    "training_set = entire_dataset[:4900]\n",
    "test_set = entire_dataset[4900:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating gzip...\n",
      "gzip accuracy: 0.2000\n",
      "Evaluating zstd...\n",
      "zstd accuracy: 0.0000\n",
      "Evaluating tokenizer...\n",
      "tokenizer accuracy: 0.1000\n",
      "\n",
      "Summary of results:\n",
      "gzip: 0.2000\n",
      "zstd: 0.0000\n",
      "tokenizer: 0.1000\n",
      "\n",
      "Best compression method: gzip with accuracy 0.2000\n",
      "Baseline for random guessing: 0.2500\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "\n",
    "\n",
    "def compress_with_gzip(text):\n",
    "    if isinstance(text, bytes):\n",
    "        return len(gzip.compress(text))\n",
    "    else:\n",
    "        return len(gzip.compress(text.encode()))\n",
    "\n",
    "\n",
    "def compress_with_zstd(text):\n",
    "    if isinstance(text, bytes):\n",
    "        return len(zstd.compress(text))\n",
    "    else:\n",
    "        return len(zstd.compress(text.encode()))\n",
    "\n",
    "\n",
    "def compress_with_tokenizer(text, tokenizer=enc):\n",
    "    if isinstance(text, bytes):\n",
    "        return len(tokenizer._encode_bytes(text))\n",
    "    else:\n",
    "        return len(tokenizer.encode(text))\n",
    "\n",
    "\n",
    "def concatenate_data(x1, x2):\n",
    "    if isinstance(x1, bytes):\n",
    "        return x1 + x2\n",
    "    else:\n",
    "        return \" \".join([x1, x2])\n",
    "\n",
    "\n",
    "def calculate_ncd(x1, x2, compression_method, tokenizer=None):\n",
    "    compression_functions = {\n",
    "        \"gzip\": compress_with_gzip,\n",
    "        \"zstd\": compress_with_zstd,\n",
    "        \"tokenizer\": lambda x: compress_with_tokenizer(x, tokenizer or enc),\n",
    "    }\n",
    "\n",
    "    if compression_method not in compression_functions:\n",
    "        raise ValueError(f\"Unknown compression method: {compression_method}\")\n",
    "\n",
    "    compress_func = compression_functions[compression_method]\n",
    "\n",
    "    Cx1 = compress_func(x1)\n",
    "    Cx2 = compress_func(x2)\n",
    "    x1x2 = concatenate_data(x1, x2)\n",
    "    Cx1x2 = compress_func(x1x2)\n",
    "\n",
    "    return (Cx1x2 - min(Cx1, Cx2)) / max(Cx1, Cx2)\n",
    "\n",
    "\n",
    "def evaluate_compression_method(method, test_samples=10):\n",
    "    \"\"\"Evaluate a compression method on the test set and return accuracy.\"\"\"\n",
    "    track_answers = []\n",
    "    for x1, x1_gt in test_set[:test_samples]:\n",
    "        distance_from_x1 = []\n",
    "\n",
    "        for x2, _ in training_set:\n",
    "            ncd = calculate_ncd(x1, x2, method)\n",
    "            distance_from_x1.append(ncd)\n",
    "\n",
    "        # Use argpartition to efficiently find the smallest element (k=1)\n",
    "        nearest_idx = np.argpartition(np.array(distance_from_x1), 0)[0]\n",
    "        # Get the class of the nearest neighbor\n",
    "        predict_class = training_set[nearest_idx][1]\n",
    "        track_answers.append((x1_gt, predict_class))\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = sum(\n",
    "        [1 for (x1_gt, predict_class) in track_answers if x1_gt == predict_class]\n",
    "    ) / len(track_answers)\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "# Evaluate all three compression methods\n",
    "compression_methods = [\"gzip\", \"zstd\", \"tokenizer\"]\n",
    "results = {}\n",
    "\n",
    "for method in compression_methods:\n",
    "    print(f\"Evaluating {method}...\")\n",
    "    accuracy = evaluate_compression_method(method)\n",
    "    results[method] = accuracy\n",
    "    print(f\"{method} accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Print summary of results\n",
    "print(\"\\nSummary of results:\")\n",
    "for method, accuracy in results.items():\n",
    "    print(f\"{method}: {accuracy:.4f}\")\n",
    "\n",
    "# Find the best method\n",
    "best_method = max(results, key=results.get)\n",
    "print(\n",
    "    f\"\\nBest compression method: {best_method} with accuracy {results[best_method]:.4f}\"\n",
    ")\n",
    "\n",
    "\n",
    "## baseline for random guessing\n",
    "print(f\"Baseline for random guessing: {1/4:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text length: 50000\n",
      "Text bytes length: 50000\n",
      "Text bytes gzip length: 21097\n",
      "Text bytes zstd length: 21660\n",
      "Text tokens length: 13411\n",
      "Text tokens bytes length: 40308\n",
      "Tokenized gzip length: 19715\n",
      "Tokenized zstd length: 20193\n"
     ]
    }
   ],
   "source": [
    "text_string = tinyshakespeare[:50000]\n",
    "print(f\"Original text length: {len(text_string)}\")\n",
    "text_bytes = text_string.encode()\n",
    "print(f\"Text bytes length: {len(text_bytes)}\")\n",
    "\n",
    "text_bytes_gzip = gzip.compress(text_bytes)\n",
    "print(f\"Text bytes gzip length: {len(text_bytes_gzip)}\")\n",
    "\n",
    "text_bytes_zstd = zstd.compress(text_bytes)\n",
    "print(f\"Text bytes zstd length: {len(text_bytes_zstd)}\")\n",
    "\n",
    "text_tokens = enc.encode(text_string)\n",
    "print(f\"Text tokens length: {len(text_tokens)}\")\n",
    "text_tokens_bytes = pickle.dumps(text_tokens)\n",
    "print(f\"Text tokens bytes length: {len(text_tokens_bytes)}\")\n",
    "\n",
    "tokenized_gzip = enc._encode_bytes(text_bytes_gzip)\n",
    "print(f\"Tokenized gzip length: {len(tokenized_gzip)}\")\n",
    "\n",
    "tokenized_zstd = enc._encode_bytes(text_bytes_zstd)\n",
    "print(f\"Tokenized zstd length: {len(tokenized_zstd)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial sequence length: 21660 | Final sequence length: 17765 | Vocab size: 2000 | Compressed ratio: 1.2192513368983957\n",
      "Initial sequence length: 21097 | Final sequence length: 17352 | Vocab size: 2000 | Compressed ratio: 1.215825265099124\n",
      "Initial sequence length: 50000 | Final sequence length: 13668 | Vocab size: 2000 | Compressed ratio: 3.6581796897863623\n"
     ]
    }
   ],
   "source": [
    "from bpe import BPETokenizer\n",
    "\n",
    "bpe_tokenizer = BPETokenizer(2000)\n",
    "\n",
    "bpe_tokenizer.train(list(text_bytes_zstd))\n",
    "encoded_bpe = bpe_tokenizer.encode_bytes(text_bytes_zstd)\n",
    "decoded_bpe = bpe_tokenizer.decode_bytes(encoded_bpe)\n",
    "assert decoded_bpe == text_bytes_zstd\n",
    "\n",
    "bpe_tokenizer = BPETokenizer(2000)\n",
    "bpe_tokenizer.train(list(text_bytes_gzip))\n",
    "encoded_bpe_gzip = bpe_tokenizer.encode_bytes(text_bytes_gzip)\n",
    "decoded_bpe_gzip = bpe_tokenizer.decode_bytes(encoded_bpe_gzip)\n",
    "assert decoded_bpe_gzip == text_bytes_gzip\n",
    "\n",
    "bpe_tokenizer = BPETokenizer(2000)\n",
    "bpe_tokenizer.train(list(text_bytes))\n",
    "encoded_bpe_bytes = bpe_tokenizer.encode_bytes(text_bytes)\n",
    "decoded_bpe_bytes = bpe_tokenizer.decode_bytes(encoded_bpe_bytes)\n",
    "assert decoded_bpe_bytes == text_bytes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
